{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQHa1kkP0F1-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistics of Text\n",
    "\n",
    "\n",
    "## CORE-UA 109, Joanna Klukowska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1542486199791,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "3mCd7_uLIlQK",
    "outputId": "46d0f741-c65d-49a2-e0c7-1af721720b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/asia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/asia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package cmudict to /home/asia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    }
   ],
   "source": [
    "# This code cell imports all the different libraries that the rest of this notebook \n",
    "# relies on. This cell has to be executed before any other cells can function. \n",
    " \n",
    "# this is a natural language processing toolking   \n",
    "import nltk\n",
    "\n",
    "# required for eliminating stopwords \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#required for breaking text into sentences \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# required for working with syllables\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict \n",
    "\n",
    "\n",
    "# This code cell also defines some additional capabilities. \n",
    "\n",
    "\n",
    "\n",
    "def count_syllables ( word ) :\n",
    "  word = word.lower()\n",
    "  if word not in d.keys() :\n",
    "    return -1\n",
    "  for x in d[word ] :\n",
    "    syl = [] \n",
    "    for  y in x :\n",
    "      if y[-1].isdigit() :\n",
    "        syl.append( y[-1] )\n",
    "  return len(syl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzigQ0c27d9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-19 09:19:40--  http://www.gutenberg.org/files/11/11-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173595 (170K) [text/plain]\n",
      "Saving to: ‘11-0.txt’\n",
      "\n",
      "11-0.txt            100%[===================>] 169.53K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2018-11-19 09:19:40 (1.15 MB/s) - ‘11-0.txt’ saved [173595/173595]\n",
      "\n",
      "--2018-11-19 09:19:40--  http://www.gutenberg.org/cache/epub/16328/pg16328.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 301063 (294K) [text/plain]\n",
      "Saving to: ‘pg16328.txt’\n",
      "\n",
      "pg16328.txt         100%[===================>] 294.01K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2018-11-19 09:19:40 (2.15 MB/s) - ‘pg16328.txt’ saved [301063/301063]\n",
      "\n",
      "--2018-11-19 09:19:40--  http://www.gutenberg.org/files/2701/2701-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1276201 (1.2M) [text/plain]\n",
      "Saving to: ‘2701-0.txt’\n",
      "\n",
      "2701-0.txt          100%[===================>]   1.22M  2.13MB/s    in 0.6s    \n",
      "\n",
      "2018-11-19 09:19:41 (2.13 MB/s) - ‘2701-0.txt’ saved [1276201/1276201]\n",
      "\n",
      "--2018-11-19 09:19:41--  http://www.gutenberg.org/files/74/74-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 428104 (418K) [text/plain]\n",
      "Saving to: ‘74-0.txt’\n",
      "\n",
      "74-0.txt            100%[===================>] 418.07K  1.70MB/s    in 0.2s    \n",
      "\n",
      "2018-11-19 09:19:41 (1.70 MB/s) - ‘74-0.txt’ saved [428104/428104]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell needs to be executed only once. \n",
    "# You can add other books to this list.  \n",
    "\n",
    "!wget http://www.gutenberg.org/files/11/11-0.txt\n",
    "!wget http://www.gutenberg.org/cache/epub/16328/pg16328.txt\n",
    "!wget http://www.gutenberg.org/files/2701/2701-0.txt \n",
    "!wget http://www.gutenberg.org/files/74/74-0.txt   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDOQ5Bn3t_aH"
   },
   "outputs": [],
   "source": [
    "\n",
    "file_AliceInWonderland = open('11-0.txt',\"r\", encoding=\"utf-8\")\n",
    "file_Beowulf = open('pg16328.txt',\"r\", encoding=\"utf-8\")\n",
    "file_MobyDick = open('2701-0.txt',\"r\", encoding=\"utf-8\")\n",
    "file_AdvOfTomSawyer = open('74-0.txt',\"r\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIJ2jQN8sHZI"
   },
   "outputs": [],
   "source": [
    "# If you wish to use a different book in this notebook, just change \n",
    "# the next line and rerun the remaining cells.\n",
    "file = file_MobyDick\n",
    "file.seek(0)\n",
    "text = file.read().replace(\"\\n\", \" \").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fs6V7c2M1Tt-"
   },
   "source": [
    "# Word Frequency\n",
    "\n",
    "\n",
    "__How often a word occurs in the given text? __\n",
    "\n",
    "To calculate it, we first need to break the text into individual words. \n",
    "\n",
    "- `split()` function works, but it does not do a great job (see the example below)\n",
    "\n",
    "- `word_tokenizer()` from the natural language processing toolkin (NLTK) does a much better job (although we will need to deal with all the _words_ that are not really words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1542492852478,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "6xazLuOeXsrb",
    "outputId": "c8ddafc7-f622-4404-bd9b-a0cca81bff6b"
   },
   "outputs": [],
   "source": [
    "phrase = \"\"\" “Let’s eat, grandpa” versus “Let’s eat grandpa”. \n",
    "              Punctuation can potentially save lives! \n",
    "         \"\"\"\n",
    "words1 = phrase.split()\n",
    "words2 = word_tokenize(phrase) \n",
    "\n",
    "for word in words1: \n",
    "  print(word, end = \"  |  \")\n",
    "print(\"\\n\\n\")\n",
    "for word in words2:\n",
    "  print(word, end = \"  |  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvkZKXJ5Zt-j"
   },
   "source": [
    "# Tallying the Words\n",
    "\n",
    "The next step is to count how many times each of the words occurs. \n",
    "\n",
    "NLTK has a function called `FreqDist()` that does exactly that. \n",
    "It returns an object that can be thought of as a list of pairs that  contain the word itself and its count. (But it can do much more. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1542492852683,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "hz9LGoDWbpPz",
    "outputId": "cc9a725f-894d-465d-94b0-1fc53db8c23e"
   },
   "outputs": [],
   "source": [
    "phrase = \"\"\" “Let’s eat, grandpa” versus “Let’s eat grandpa”. \n",
    "              Punctuation can potentially save lives! \n",
    "         \"\"\"\n",
    "words2 = word_tokenize(phrase) \n",
    "\n",
    "frequencies = nltk.FreqDist(words2)\n",
    "\n",
    "for key in frequencies: \n",
    "  print (key, \" : \", frequencies[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pi2FiIsIcj6D"
   },
   "source": [
    "# Word Frequencies in Our Book - Take 1 \n",
    "\n",
    "We will put it all together to find out the list of the most-used words in our book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2951,
     "status": "ok",
     "timestamp": 1542492854966,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "Akli1Gpx2r78",
    "outputId": "49152191-aaf8-4043-f490-ac044053399b"
   },
   "outputs": [],
   "source": [
    "# break the text of the book into individual words/tokens\n",
    "words = word_tokenize(text )\n",
    "\n",
    "# calculate the number of occurences of each work/token\n",
    "freq = nltk.FreqDist(words) \n",
    "\n",
    "# plot the 20 most popular ones \n",
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vy_GnMUZd3TV"
   },
   "source": [
    "__What is the problem with the above date?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WEjgH__eDIm"
   },
   "source": [
    "# Word Frequencies in Our Book - Take 2\n",
    "\n",
    "We need to remove the things from the list of words that are not actual words. \n",
    "This is often refered to as _cleaning_ of the data set. \n",
    "We will do this in two steps:\n",
    "- remove the _stop words_ - these are frequently occuring, usually short words that do not have much meaning, like \"to\", \"a\", ... (NLTK has a list of those)\n",
    "- remove all the punctuation marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 165687,
     "status": "ok",
     "timestamp": 1542493017740,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "NKvWkIDU1_Sb",
    "outputId": "f84bdcc4-0374-4ccf-aac3-c11b3e0283fd"
   },
   "outputs": [],
   "source": [
    "# make a copy of the words and call it clean_words\n",
    "clean_words = words[0:]\n",
    "\n",
    "# get the list of stop words from the nltk \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for word in words:\n",
    "  # remove all stop words from the clean_words list \n",
    "  if word in stop_words:\n",
    "    clean_words.remove(word)\n",
    "  # remove short \n",
    "  if not word.isalpha()  :\n",
    "    clean_words.remove(word)\n",
    "    \n",
    "freq_clean = nltk.FreqDist(clean_words)\n",
    "\n",
    "freq_clean.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arB6XYEWiXIy"
   },
   "source": [
    "# Sentences\n",
    "\n",
    "In some cases, we may want to obtain individual sentences from the text. \n",
    "This can be done with the `sent_tokenize()` function from the NLTK (`sent` stands for _sentence_, not sending something). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 165663,
     "status": "ok",
     "timestamp": 1542493017749,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "0J8b5Pcki2oK",
    "outputId": "f6504081-5472-4448-a8bc-30b392ec4105"
   },
   "outputs": [],
   "source": [
    "phrase = \"\"\" “Let’s eat, grandpa!” versus “Let’s eat grandpa!”. \n",
    "              Punctuation can potentially save lives! \n",
    "         \"\"\"\n",
    "sents = sent_tokenize(phrase) \n",
    "\n",
    "for sent in sents: \n",
    "  print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YO369bYRjmLC"
   },
   "source": [
    "# Breaking Our Book into Sentences\n",
    "\n",
    "Using the same idea for our book, we can easily figure out how many sentences there are in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 165834,
     "status": "ok",
     "timestamp": 1542493017953,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "Ff-eIxlN_kFy",
    "outputId": "8bc79e76-71f5-4c34-8e93-f26cc290c2b4"
   },
   "outputs": [],
   "source": [
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"There are\", len(sentences), \"sentences in this book.\")\n",
    "\n",
    "print(\"The fifth sentence is: \\n\", sentences[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fD_Icbr7klsN"
   },
   "source": [
    "# Lengths of Sentences - Challenge\n",
    "\n",
    "Write a program that given a list of sentences calculates the length of the longest, shortest and the average sentence. \n",
    "\n",
    "The length is measured in the number of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 165812,
     "status": "ok",
     "timestamp": 1542493017957,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "raO-Hf78F95N",
    "outputId": "8d27bae0-25f3-4349-f627-c0fb0c07953e"
   },
   "outputs": [],
   "source": [
    "# create a list of lengths of words\n",
    "lengths = []\n",
    "for s in sentences :\n",
    "  # strip any white-space characters from the start and end of sentences\n",
    "  lengths.append( len(s.strip() ) ) \n",
    "  \n",
    "# calculate the stats \n",
    "average_sentence_length = sum(lengths)/ len(lengths) \n",
    "max_sentence_length = max(lengths)\n",
    "min_sentence_length = min(lengths)\n",
    "\n",
    "# print the results \n",
    "print( average_sentence_length)\n",
    "print( max_sentence_length)\n",
    "print( min_sentence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uizEUcklluK0"
   },
   "source": [
    "# Counting Syllables \n",
    "\n",
    "For some of the readability measures we needed to be able to determine the number of syllables in a word (or sentence or text). \n",
    "\n",
    "The top of this notebook, defines a function that calculates the number of syllables given a word. It uses CMU pronounciation dictionary to accomplish this task. \n",
    "\n",
    "The function returns -1 if the word does not appear in the CMU dictionary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 165790,
     "status": "ok",
     "timestamp": 1542493017961,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "lIfZ7MRmHLN0",
    "outputId": "3ca21efc-0902-404c-f369-8530d9bba734"
   },
   "outputs": [],
   "source": [
    "word = \"cat\"\n",
    "# word = \"statistics\"\n",
    "# word = \"antidisestablishmentarianism\"\n",
    "# word = \"Klukowska\"\n",
    "count = count_syllables ( word )\n",
    "print (word, \"has\", count, \"syllable(s)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-TB3WO8nAu2"
   },
   "source": [
    "# Counting Syllables - Challenges\n",
    "\n",
    "- Write a program that determines the number of syllables in the entire text of our book. Determine the word with the largest number of syllables and the average number of syllables per word. \n",
    "\n",
    "- Write a program that determines the number of syllables in each of the sentences. (Be careful trying to print these reseults - for long books it may take a lot of time.)\n",
    "\n",
    "- Write a program that determines the average number of syllables per sentence in our book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166318,
     "status": "ok",
     "timestamp": 1542493018511,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "0ieL-EozL6Bo",
    "outputId": "b77bace6-9ac0-47e9-d178-9ae86541b185"
   },
   "outputs": [],
   "source": [
    "count_syl = 0\n",
    "count_words = 0 \n",
    "max_syl = 0\n",
    "\n",
    "\n",
    "for word in clean_words:\n",
    "  c = count_syllables( word ) \n",
    "  if (c > 0): \n",
    "    count_syl = count_syl + c \n",
    "    count_words = count_words + 1\n",
    "    if max_syl < c :\n",
    "      max_syl = c\n",
    "\n",
    "print (\"syllables found:\", count_syl)\n",
    "print (\"words counted:\", count_words)\n",
    "print (\"words ignored:\", len(clean_words) - count_words)\n",
    "print (\"syllables per word on average:\", count_syl / count_words)\n",
    "print (\"the largest number of syllables in one word:\", max_syl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hk-l5YS8pfSa"
   },
   "source": [
    "# Flesch-Kincaid Index\n",
    "\n",
    "Recall that Flesch-Kincaid Index is used to asses the difficulty of reading materials. \n",
    "\n",
    "The result is a number that corresponds with a US grade level.\n",
    "\n",
    "It is calculated using the following formula:\n",
    "\n",
    "$$0.39(\\dfrac{\\text{total words}}{\\text{total sentences}})  + 11.8 (\\dfrac{\\text{total syllables}}{\\text{total words}}) - 15.59$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 166297,
     "status": "ok",
     "timestamp": 1542493018515,
     "user": {
      "displayName": "Joanna Klukowska",
      "photoUrl": "https://lh3.googleusercontent.com/-1DIfUVDYRvs/AAAAAAAAAAI/AAAAAAAAAGM/Ji3WhXx6GCE/s64/photo.jpg",
      "userId": "03185644073057106538"
     },
     "user_tz": 300
    },
    "id": "zNPv_tcMMlSB",
    "outputId": "d69c4d83-1bc7-4a64-a291-d64e3a6e533a"
   },
   "outputs": [],
   "source": [
    "tot_sentences = len(sentences)\n",
    "tot_words = count_words\n",
    "tot_syllables = count_syl\n",
    "\n",
    "fk_index = 0.39 * tot_words/tot_sentences \\\n",
    "      + 11.8 * tot_syllables/tot_words \\\n",
    "      - 15.59\n",
    "\n",
    "print(\"The Flesch-Kincaid index is\", fk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BBCJt28rxfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "14-statistics_of_text.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
